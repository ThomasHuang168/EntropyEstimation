{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-327be51fc1c9>, line 18)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-327be51fc1c9>\"\u001b[0;36m, line \u001b[0;32m18\u001b[0m\n\u001b[0;31m    EPS = np.finfo(float).epsdef nearest_distances(X, k=1):\u001b[0m\n\u001b[0m                                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Non-parametric computation of entropy and mutual-information\n",
    "Adapted by G Varoquaux for code created by R Brette, itself\n",
    "from several papers (see in the code).\n",
    "These computations rely on nearest-neighbor statistics\n",
    "'''\n",
    "import numpy as np\n",
    "\n",
    "from scipy.special import gamma,psi\n",
    "from scipy import ndimage\n",
    "from scipy.linalg import det\n",
    "from numpy import pi\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "__all__=['entropy', 'mutual_information', 'entropy_gaussian']\n",
    "\n",
    "EPS = np.finfo(float).eps\n",
    "\n",
    "\n",
    "\n",
    "def nearest_distances(X, k=1):\n",
    "    '''\n",
    "    X = array(N,M)\n",
    "    N = number of points\n",
    "    M = number of dimensions\n",
    "    returns the distance to the kth nearest neighbor for every point in X\n",
    "    '''\n",
    "    knn = NearestNeighbors(n_neighbors=k)\n",
    "    knn.fit(X)\n",
    "    d, _ = knn.kneighbors(X) # the first nearest neighbor is itself\n",
    "    return d[:, -1] # returns the distance to the kth nearest neighbor\n",
    "\n",
    "\n",
    "def entropy_gaussian(C):\n",
    "    '''\n",
    "    Entropy of a gaussian variable with covariance matrix C\n",
    "    '''\n",
    "    if np.isscalar(C): # C is the variance\n",
    "        return .5*(1 + np.log(2*pi)) + .5*np.log(C)\n",
    "    else:\n",
    "        n = C.shape[0] # dimension\n",
    "        return .5*n*(1 + np.log(2*pi)) + .5*np.log(abs(det(C)))\n",
    "\n",
    "\n",
    "def entropy(X, k=1):\n",
    "    ''' Returns the entropy of the X.\n",
    "    Parameters\n",
    "    ===========\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        The data the entropy of which is computed\n",
    "    k : int, optional\n",
    "        number of nearest neighbors for density estimation\n",
    "    Notes\n",
    "    ======\n",
    "    Kozachenko, L. F. & Leonenko, N. N. 1987 Sample estimate of entropy\n",
    "    of a random vector. Probl. Inf. Transm. 23, 95-101.\n",
    "    See also: Evans, D. 2008 A computationally efficient estimator for\n",
    "    mutual information, Proc. R. Soc. A 464 (2093), 1203-1215.\n",
    "    and:\n",
    "    Kraskov A, Stogbauer H, Grassberger P. (2004). Estimating mutual\n",
    "    information. Phys Rev E 69(6 Pt 2):066138.\n",
    "    '''\n",
    "\n",
    "    # Distance to kth nearest neighbor\n",
    "    r = nearest_distances(X, k) # squared distances\n",
    "    n, d = X.shape\n",
    "    volume_unit_ball = (pi**(.5*d)) / gamma(.5*d + 1)\n",
    "    '''\n",
    "    F. Perez-Cruz, (2008). Estimation of Information Theoretic Measures\n",
    "    for Continuous Random Variables. Advances in Neural Information\n",
    "    Processing Systems 21 (NIPS). Vancouver (Canada), December.\n",
    "    return d*mean(log(r))+log(volume_unit_ball)+log(n-1)-log(k)\n",
    "    '''\n",
    "    return (d*np.mean(np.log(r + np.finfo(X.dtype).eps))\n",
    "            + np.log(volume_unit_ball) + psi(n) - psi(k))\n",
    "\n",
    "\n",
    "def mutual_information(variables, k=1):\n",
    "    '''\n",
    "    Returns the mutual information between any number of variables.\n",
    "    Each variable is a matrix X = array(n_samples, n_features)\n",
    "    where\n",
    "      n = number of samples\n",
    "      dx,dy = number of dimensions\n",
    "    Optionally, the following keyword argument can be specified:\n",
    "      k = number of nearest neighbors for density estimation\n",
    "    Example: mutual_information((X, Y)), mutual_information((X, Y, Z), k=5)\n",
    "    '''\n",
    "    if len(variables) < 2:\n",
    "        raise AttributeError(\n",
    "                \"Mutual information must involve at least 2 variables\")\n",
    "    all_vars = np.hstack(variables)\n",
    "    return (sum([entropy(X, k=k) for X in variables])\n",
    "            - entropy(all_vars, k=k))\n",
    "\n",
    "\n",
    "def mutual_information_2d(x, y, sigma=1, normalized=False):\n",
    "    \"\"\"\n",
    "    Computes (normalized) mutual information between two 1D variate from a\n",
    "    joint histogram.\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : 1D array\n",
    "        first variable\n",
    "    y : 1D array\n",
    "        second variable\n",
    "    sigma: float\n",
    "        sigma for Gaussian smoothing of the joint histogram\n",
    "    Returns\n",
    "    -------\n",
    "    nmi: float\n",
    "        the computed similariy measure\n",
    "    \"\"\"\n",
    "    bins = (256, 256)\n",
    "\n",
    "    jh = np.histogram2d(x, y, bins=bins)[0]\n",
    "\n",
    "    # smooth the jh with a gaussian filter of given sigma\n",
    "    ndimage.gaussian_filter(jh, sigma=sigma, mode='constant',\n",
    "                                 output=jh)\n",
    "\n",
    "    # compute marginal histograms\n",
    "    jh = jh + EPS\n",
    "    sh = np.sum(jh)\n",
    "    jh = jh / sh\n",
    "    s1 = np.sum(jh, axis=0).reshape((-1, jh.shape[0]))\n",
    "    s2 = np.sum(jh, axis=1).reshape((jh.shape[1], -1))\n",
    "\n",
    "    # Normalised Mutual Information of:\n",
    "    # Studholme,  jhill & jhawkes (1998).\n",
    "    # \"A normalized entropy measure of 3-D medical image alignment\".\n",
    "    # in Proc. Medical Imaging 1998, vol. 3338, San Diego, CA, pp. 132-143.\n",
    "    if normalized:\n",
    "        mi = ((np.sum(s1 * np.log(s1)) + np.sum(s2 * np.log(s2)))\n",
    "                / np.sum(jh * np.log(jh))) - 1\n",
    "    else:\n",
    "        mi = ( np.sum(jh * np.log(jh)) - np.sum(s1 * np.log(s1))\n",
    "               - np.sum(s2 * np.log(s2)))\n",
    "\n",
    "    return mi\n",
    "\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# Tests\n",
    "\n",
    "def test_entropy():\n",
    "    # Testing against correlated Gaussian variables\n",
    "    # (analytical results are known)\n",
    "    # Entropy of a 3-dimensional gaussian variable\n",
    "    rng = np.random.RandomState(0)\n",
    "    n = 50000\n",
    "    d = 3\n",
    "    P = np.array([[1, 0, 0], [0, 1, .5], [0, 0, 1]])\n",
    "    C = np.dot(P, P.T)\n",
    "    Y = rng.randn(d, n)\n",
    "    X = np.dot(P, Y)\n",
    "    H_th = entropy_gaussian(C)\n",
    "    H_est = entropy(X.T, k=5)\n",
    "    # Our estimated entropy should always be less that the actual one\n",
    "    # (entropy estimation undershoots) but not too much\n",
    "    np.testing.assert_array_less(H_est, H_th)\n",
    "    np.testing.assert_array_less(.9*H_th, H_est)\n",
    "\n",
    "\n",
    "def test_mutual_information():\n",
    "    # Mutual information between two correlated gaussian variables\n",
    "    # Entropy of a 2-dimensional gaussian variable\n",
    "    n = 50000\n",
    "    rng = np.random.RandomState(0)\n",
    "    #P = np.random.randn(2, 2)\n",
    "    P = np.array([[1, 0], [0.5, 1]])\n",
    "    C = np.dot(P, P.T)\n",
    "    U = rng.randn(2, n)\n",
    "    Z = np.dot(P, U).T\n",
    "    X = Z[:, 0]\n",
    "    X = X.reshape(len(X), 1)\n",
    "    Y = Z[:, 1]\n",
    "    Y = Y.reshape(len(Y), 1)\n",
    "    # in bits\n",
    "    MI_est = mutual_information((X, Y), k=5)\n",
    "    MI_th = (entropy_gaussian(C[0, 0])\n",
    "             + entropy_gaussian(C[1, 1])\n",
    "             - entropy_gaussian(C)\n",
    "            )\n",
    "    # Our estimator should undershoot once again: it will undershoot more\n",
    "    # for the 2D estimation that for the 1D estimation\n",
    "    print  MI_est, MI_th\n",
    "    np.testing.assert_array_less(MI_est, MI_th)\n",
    "    np.testing.assert_array_less(MI_th, MI_est  + .3)\n",
    "\n",
    "\n",
    "def test_degenerate():\n",
    "    # Test that our estimators are well-behaved with regards to\n",
    "    # degenerate solutions\n",
    "    rng = np.random.RandomState(0)\n",
    "    x = rng.randn(50000)\n",
    "    X = np.c_[x, x]\n",
    "    assert np.isfinite(entropy(X))\n",
    "    assert np.isfinite(mutual_information((x[:, np.newaxis],\n",
    "                                           x[:,  np.newaxis])))\n",
    "    assert 2.9 < mutual_information_2d(x, x) < 3.1\n",
    "\n",
    "\n",
    "def test_mutual_information_2d():\n",
    "    # Mutual information between two correlated gaussian variables\n",
    "    # Entropy of a 2-dimensional gaussian variable\n",
    "    n = 50000\n",
    "    rng = np.random.RandomState(0)\n",
    "    #P = np.random.randn(2, 2)\n",
    "    P = np.array([[1, 0], [.9, .1]])\n",
    "    C = np.dot(P, P.T)\n",
    "    U = rng.randn(2, n)\n",
    "    Z = np.dot(P, U).T\n",
    "    X = Z[:, 0]\n",
    "    X = X.reshape(len(X), 1)\n",
    "    Y = Z[:, 1]\n",
    "    Y = Y.reshape(len(Y), 1)\n",
    "    # in bits\n",
    "    MI_est = mutual_information_2d(X.ravel(), Y.ravel())\n",
    "    MI_th = (entropy_gaussian(C[0, 0])\n",
    "             + entropy_gaussian(C[1, 1])\n",
    "             - entropy_gaussian(C)\n",
    "            )\n",
    "    print  MI_est, MI_th\n",
    "    # Our estimator should undershoot once again: it will undershoot more\n",
    "    # for the 2D estimation that for the 1D estimation\n",
    "    np.testing.assert_array_less(MI_est, MI_th)\n",
    "    np.testing.assert_array_less(MI_th, MI_est  + .2)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Run our tests\n",
    "    test_entropy()\n",
    "    test_mutual_information()\n",
    "    test_degenerate()\n",
    "test_mutual_information_2d()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
